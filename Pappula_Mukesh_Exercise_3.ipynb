{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mukeshreddy3699/Mukesh_INFO5731/blob/main/Pappula_Mukesh_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "d4beccfd-c9bb-46b2-c7ab-7d364b9be2ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n1.Bag of words: turns each review into important words used.\\nwhy it\\'s helpful: High-frequency words in positive reviews might include terms like \"excellent,\" or \"amazing,\" while negative reviews may contain words like \"disappointing,\" \"problems,\" or \"poor\"\\n2.TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF weights, emphasizing the importance of words that rare in the entire dataset but frequent in a specific review.similar to bow.\\nwhy its helpful: It highlights unique words in reviews, showing what specific things about the product customers find important.\\n3.Emoticons and Emoji Features: This feature looks for symbols like üòä or üòÆ\\u200düí® or üôÉ in review.\\nwhy its helpful:Emojis and emoticons often express emotions clearly.üòä as positive sentiments ,üôÉ as negative sentiments.\\n4.N-grams (2-grams) Features:This feature looks at pairs of consecutive words (like ‚Äúnot good‚Äù).\\nwhy its helpful:Some sentiments may be conveyed through specific phrases, and capturing 2-grams can provide context that single words may miss.\\n5.Product-specific Lexicon Features:Counting occurrences of product-related words in sentiment lexicons\\nwhys its helpful:Understanding sentiments related to specific product aspects allows businesses to pinpoint areas for improvement or highlight strengths.\\n6.Part-of-Speech (POS) Tags Features:Tagging words with their parts of speech provides insights into the grammatical structure of reviews.\\nwhy its helpful:Identifying adjectives in positive reviews or negative reviews helps understand the linguistic patterns associated with sentiments.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "1.Bag of words: turns each review into important words used.\n",
        "why it's helpful: High-frequency words in positive reviews might include terms like \"excellent,\" or \"amazing,\" while negative reviews may contain words like \"disappointing,\" \"problems,\" or \"poor\"\n",
        "2.TF-IDF (Term Frequency-Inverse Document Frequency): TF-IDF weights, emphasizing the importance of words that rare in the entire dataset but frequent in a specific review.similar to bow.\n",
        "why its helpful: It highlights unique words in reviews, showing what specific things about the product customers find important.\n",
        "3.Emoticons and Emoji Features: This feature looks for symbols like üòä or üòÆ‚Äçüí® or üôÉ in review.\n",
        "why its helpful:Emojis and emoticons often express emotions clearly.üòä as positive sentiments ,üôÉ as negative sentiments.\n",
        "4.N-grams (2-grams) Features:This feature looks at pairs of consecutive words (like ‚Äúnot good‚Äù).\n",
        "why its helpful:Some sentiments may be conveyed through specific phrases, and capturing 2-grams can provide context that single words may miss.\n",
        "5.Product-specific Lexicon Features:Counting occurrences of product-related words in sentiment lexicons\n",
        "whys its helpful:Understanding sentiments related to specific product aspects allows businesses to pinpoint areas for improvement or highlight strengths.\n",
        "6.Part-of-Speech (POS) Tags Features:Tagging words with their parts of speech provides insights into the grammatical structure of reviews.\n",
        "why its helpful:Identifying adjectives in positive reviews or negative reviews helps understand the linguistic patterns associated with sentiments.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUmMOAld5u5S",
        "outputId": "6ee052a1-58eb-4e34-c20b-dabc4ad6100c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7692975-6a38-4401-c366-d482cb5cc217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Count Features:\n",
            "   also  football  games  john  likes  mary  movies  watch\n",
            "0     0         0      0     1      2     1       2      1\n",
            "1     1         1      1     0      1     1       0      1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "Ref_text = [\n",
        "    \" John likes to watch movies. Mary likes movies too üòä \",\n",
        "    \" Mary also likes to watch football games üòÆ‚Äçüí® \"\n",
        "]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def custom_preprocess(input_text):\n",
        "    processed_text = input_text.lower()\n",
        "    tokenized_text = word_tokenize(processed_text)\n",
        "    filtered_tokens = [word for word in tokenized_text if word not in stop_words]  #Remove stopwords\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "custom_processed_text = [custom_preprocess(text) for text in Ref_text]\n",
        "\n",
        "word_count_vectorizer = CountVectorizer()\n",
        "word_count_features = word_count_vectorizer.fit_transform(custom_processed_text)\n",
        "word_count_df = pd.DataFrame(word_count_features.toarray(), columns=word_count_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nWord Count Features:\")\n",
        "print(word_count_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "newtfidf_model = TfidfVectorizer()    #for tf-idf functionality\n",
        "newtfidf_data = newtfidf_model.fit_transform(custom_processed_text)\n",
        "newtfidf_dataframe = pd.DataFrame(newtfidf_data.toarray(), columns=newtfidf_model.get_feature_names_out())\n",
        "print(\"\\nNew TF-IDF Features:\")\n",
        "print(newtfidf_dataframe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82aJ8HxknByQ",
        "outputId": "030c4cc1-3574-4e25-fc10-a5f6464491dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New TF-IDF Features:\n",
            "       also  football     games      john     likes      mary    movies  \\\n",
            "0  0.000000  0.000000  0.000000  0.352728  0.501938  0.250969  0.705457   \n",
            "1  0.470426  0.470426  0.470426  0.000000  0.334712  0.334712  0.000000   \n",
            "\n",
            "      watch  \n",
            "0  0.250969  \n",
            "1  0.334712  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "def emojis(text):\n",
        "    emoji_pattern = re.compile(\"[\"u\"\\U0001F600-\\U0001F64F\" \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.findall(text)\n",
        "emoji = [emojis(text) for text in Ref_text]\n",
        "emoji_df = pd.DataFrame({'Emoji Features': emoji})\n",
        "print(\"\\nEmoticon and Emoji Features:\")\n",
        "print(emoji_df)"
      ],
      "metadata": {
        "id": "m5XnwVQHnE16",
        "outputId": "7a305b25-b6a1-4ad1-8210-5d7e020c889d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Emoticon and Emoji Features:\n",
            "  Emoji Features\n",
            "0            [üòä]\n",
            "1            [üòÆ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
        "ngram_features = vectorizer.fit_transform(custom_processed_text)\n",
        "features_df = pd.DataFrame(ngram_features.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"\\n3-gram Features:\")\n",
        "print(features_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN5kZszRpmOF",
        "outputId": "ed1b9819-6ec3-4870-b7b8-bccab49f5650"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3-gram Features:\n",
            "   also likes watch  john likes watch  likes watch football  \\\n",
            "0                 0                 1                     0   \n",
            "1                 1                 0                     1   \n",
            "\n",
            "   likes watch movies  mary also likes  mary likes movies  movies mary likes  \\\n",
            "0                   1                0                  1                  1   \n",
            "1                   0                1                  0                  0   \n",
            "\n",
            "   watch football games  watch movies mary  \n",
            "0                     0                  1  \n",
            "1                     1                  0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words = [\"movies, football\"]\n",
        "negative_words = [\"likes, also, watch\"]\n",
        "\n",
        "def count_words(text, lexicon):           #Function to count occurrences of words\n",
        "    words = text.split()\n",
        "    count = sum(1 for word in words if word in lexicon)\n",
        "    return count\n",
        "positive_words_features = [count_words(text, positive_words) for text in custom_processed_text]\n",
        "negative_words_features = [count_words(text, negative_words) for text in custom_processed_text]\n",
        "sentiment_df = pd.DataFrame({'Positive sentiment Features': positive_words_features,\n",
        "                           'Negative sentiment Features': negative_words_features})\n",
        "print(\"\\nSentiment Features:\")\n",
        "print(sentiment_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieg4aFyUpufJ",
        "outputId": "8374b4e1-659d-4322-acb6-4f3f8a6dae6b"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentiment Features:\n",
            "   Positive sentiment Features  Negative sentiment Features\n",
            "0                            0                            0\n",
            "1                            0                            0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part-of-Speech (POS) Tags Features:\n",
        "Ref_text = [\n",
        "  \" John likes to watch movies. Mary likes movies too üòä \",\n",
        "    \" Mary also likes to watch football games üòÆ‚Äçüí® \"\n",
        "]\n",
        "pos_tags = [nltk.pos_tag(word_tokenize(text)) for text in Ref_text]\n",
        "print(\"\\nPart-of-Speech (POS) Tags:\")\n",
        "for tags in pos_tags:\n",
        "    print(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf5OP7rfqQyC",
        "outputId": "7a91ac6d-c28b-46b0-d0a2-3aed18e6fc27"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Part-of-Speech (POS) Tags:\n",
            "[('John', 'NNP'), ('likes', 'VBZ'), ('to', 'TO'), ('watch', 'VB'), ('movies', 'NNS'), ('.', '.'), ('Mary', 'NNP'), ('likes', 'JJ'), ('movies', 'NNS'), ('too', 'RB'), ('üòä', 'JJ')]\n",
            "[('Mary', 'NNP'), ('also', 'RB'), ('likes', 'VBZ'), ('to', 'TO'), ('watch', 'VB'), ('football', 'NN'), ('games', 'NNS'), ('üòÆ\\u200düí®', 'VBP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "import numpy as np\n",
        "\n",
        "#Sample texts and labels\n",
        "texts = [\n",
        "    \"I'm so excited for the weekend! üòä\",\n",
        "    \"This pizza is absolutely delicious! üçïüòã\",\n",
        "    \"What a beautiful day it is outside! üåûüòä\",\n",
        "    \"I can't believe I finally finished my project! üéâüòÑ\",\n",
        "]\n",
        "\n",
        "labels = [1, 0, 2, 1]  #Adjusted to match the 4 texts\n",
        "\n",
        "emojis = ['üòä', 'üòÉ', 'üò°', 'üéâ', 'üî•', 'üò§', 'üåû', 'üòå']           #Emojis\n",
        "\n",
        "emoji_features = np.array([[1 if emoji in text else 0 for emoji in emojis] for text in texts]) #emoji feature\n",
        "vectorizer = CountVectorizer()\n",
        "word_count_features = vectorizer.fit_transform(texts).toarray()\n",
        "combined_features = np.hstack([word_count_features, emoji_features])      #word count and emoji features\n",
        "selected_indices = np.argsort(selector.scores_)[::-1]\n",
        "ranked_features = [f\"Feature {i+1}\" for i in selected_indices]\n",
        "\n",
        "print(\"Ranked Feature Indices:\")          #ranked feature indices\n",
        "for i, index in enumerate(selected_indices):\n",
        "    print(f\"{i}: Feature {index + 1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy7fQirP8kz0",
        "outputId": "f80d32f2-26f6-404f-b78a-79fa7d0f8049"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Feature Indices:\n",
            "0: Feature 29\n",
            "1: Feature 14\n",
            "2: Feature 2\n",
            "3: Feature 3\n",
            "4: Feature 4\n",
            "5: Feature 5\n",
            "6: Feature 6\n",
            "7: Feature 7\n",
            "8: Feature 8\n",
            "9: Feature 9\n",
            "10: Feature 10\n",
            "11: Feature 11\n",
            "12: Feature 12\n",
            "13: Feature 13\n",
            "14: Feature 15\n",
            "15: Feature 28\n",
            "16: Feature 16\n",
            "17: Feature 17\n",
            "18: Feature 18\n",
            "19: Feature 19\n",
            "20: Feature 20\n",
            "21: Feature 21\n",
            "22: Feature 22\n",
            "23: Feature 23\n",
            "24: Feature 24\n",
            "25: Feature 25\n",
            "26: Feature 26\n",
            "27: Feature 27\n",
            "28: Feature 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ZyW1rn4Z4d",
        "outputId": "03ed76d8-8e29-499c-a9bc-d303378deffd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22b2b049-ad9e-46be-a3a2-38ec8e441b9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ranked Texts by Similarity:\n",
            "                                                Text  Similarity\n",
            "0                  I'm so excited for the weekend! üòä    0.520253\n",
            "1             What a beautiful day it is outside! üåûüòä    0.406379\n",
            "2  Feeling grateful for all the wonderful friends...    0.359791\n",
            "3             This pizza is absolutely delicious! üçïüòã    0.306172\n",
            "4  I can't believe I finally finished my project! üéâüòÑ    0.052726\n"
          ]
        }
      ],
      "source": [
        "#Sample texts\n",
        "Ref_texts = [\n",
        "    \"I'm so excited for the weekend! üòä\",\n",
        "    \"This pizza is absolutely delicious! üçïüòã\",\n",
        "    \"What a beautiful day it is outside! üåûüòä\",\n",
        "    \"I can't believe I finally finished my project! üéâüòÑ\",\n",
        "    \"Feeling grateful for all the wonderful friends in my life! üòä‚ù§Ô∏è\"\n",
        "]\n",
        "\n",
        "#Query\n",
        "query_text = \"Looking for a delightful weekend with great food and friends.\"\n",
        "\n",
        "#BERT model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "#embeddings for the query and the sample texts\n",
        "query_embedding = model.encode(query_text, convert_to_tensor=True)\n",
        "text_embeddings = model.encode(Ref_texts, convert_to_tensor=True)\n",
        "\n",
        "similarity_scores = util.pytorch_cos_sim(query_embedding, text_embeddings)[0]\n",
        "results_df = pd.DataFrame({'Text': Ref_texts, 'Similarity': similarity_scores.tolist()})\n",
        "results_df = results_df.sort_values(by='Similarity', ascending=False)\n",
        "\n",
        "#Print ranked texts by similarity\n",
        "print(\"\\nRanked Texts by Similarity:\")\n",
        "print(results_df.reset_index(drop=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here: The time provided was not sufficient to fully explore implementing the techniques on real datasets.Extracting features from text provided good exposure to techniques like bag-of-words, TF-IDF, n-grams and BERT embeddings.\n",
        "Understanding how to generate numerical representations from text was very beneficial.the exercises served as a good introduction to feature engineering for NLP. Hands-on work with real data would further improve learning.\n",
        "Feature extraction is an integral part of the NLP workflow.\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}