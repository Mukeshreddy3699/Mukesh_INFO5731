{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wknRrNHwjodK"
      },
      "source": [
        "#**ASSIGNMENT 1**\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "---\n",
        "* Please download the provided IPython Notebook (ipynb) file and open it in Google Colab. Once opened, enter your code in the same file directly beneath the relevant question's code block.\n",
        "* Insert a text block below your code to briefly explain it, mentioning any libraries or functions utilized. Answer the questions in brief with examples.\n",
        "\n",
        "* Submit  \n",
        "1. The IPython Notebook (ipynb) file.  \n",
        "2. A PDF version of the notebook (converted from ipynb).\n",
        "3. The similarity score should be less than 15%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0MGUnuLnXue"
      },
      "source": [
        "#**Task 1: Tokenization & & Sentence Segmentation(20%)**\n",
        "(refer to the spacy tokenization concept which is explained after Question1 in activity-1)\n",
        "\n",
        "**Question -1:**\n",
        "\n",
        "##How can incorporating contextual information beyond single words enhance tokenization and improve performance in downstream tasks such as machine translation or question answering?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9jooJ_D1UQp"
      },
      "source": [
        "Answer Here:Employing context outside of individual words gives strength to tokenization by allowing models to identify words within their context, providing improved performance for tasks like machine translation and question answering. Tokenization without context will incorrectly split multi-word units like \"New York\" or misinterpret such words as \"bank,\" either a bank as in an institution or the riverbank. By being context-aware, tokenization prevents words from being misclassified and wrongly interpreted on the basis of meaning. This is important with languages that have heavily convoluted word construction, such that dismantling words incorrectly would transform their entire meaning. When it comes to machine translation, context helps maintain the correct structure and meaning of sentences across languages. Context interpretation in question-answering systems allows the model to accurately select relevant information. Context-aware tokenization also improves sentence segmentation by distinguishing between abbreviations and actual sentence breaks. Overall, by maintaining word relationships, contextual tokenization improves the accuracy and performance of NLP models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIl7PgRaeGjn"
      },
      "source": [
        "#####**Question 2**\n",
        "##Develop a tokenizer that considers contractions like \"can't\" and \"doesn't\", splitting them into their constituent words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ_rfINQfbA2",
        "outputId": "bdbd0c06-0edb-4f17-d66f-ad7cf15826d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I', 'would', 'not', 'have', 'guessed', 'that', 'she', 'would', 'leave', 'early,', 'but', 'it', 'does', 'not', 'seem', 'surprising', 'now.', \"They're\", 'not', 'coming', 'either,', 'are', 'they?']\n"
          ]
        }
      ],
      "source": [
        "text=\"I wouldn't've guessed that she'd leave early, but it doesn't seem surprising now. They're not coming either, are they?.\"\n",
        "#CODE HERE\n",
        "import re\n",
        "\n",
        "def expand_shortened_words(sentence):\n",
        "    word_expansions = {\n",
        "        \"can't\": \"can not\", \"doesn't\": \"does not\", \"wouldn't've\": \"would not have\",\n",
        "        \"she'd\": \"she would\", \"they're\": \"they are\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
        "        \"I'm\": \"I am\", \"it's\": \"it is\", \"he's\": \"he is\", \"she's\": \"she is\",\n",
        "        \"didn't\": \"did not\", \"haven't\": \"have not\", \"hasn't\": \"has not\"\n",
        "    }\n",
        "\n",
        "    for short_form, full_form in word_expansions.items():\n",
        "        sentence = re.sub(r'\\b' + re.escape(short_form) + r'\\b', full_form, sentence)\n",
        "\n",
        "    return sentence.split()\n",
        "input_text = \"I wouldn't've guessed that she'd leave early, but it doesn't seem surprising now. They're not coming either, are they?\"\n",
        "result_tokens = expand_shortened_words(input_text)\n",
        "print(result_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqhZaeV-Jrix"
      },
      "source": [
        "###This Python script consolidates contractions within sentence first, after that word division of sentence is done. The function expand_shortened_words() makes use of wordbook that translates shortened wordings into full wordings by using re.sub() to make word division possible accurately. With input sentence \"I wouldn't've guessed that she'd leave early, but it doesn't seem surprising now.\", it converts \"wouldn't've\" into \"would not have\" and \"she'd\" into \"she would\" to give well-cleared word lists that can undergo subsequent processes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BRekASOnmsz"
      },
      "source": [
        "\n",
        "**Question 3:**\n",
        "##Implement a Python script to remove Twitter username handles from a given twitter text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MD-tQZKsCkL",
        "outputId": "638bd5f9-0c04-4cfe-a66b-413d69fbe68a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Awesome brainstorming session with  and ! Excited for what‚Äôs next. Appreciate the valuable input  üí°üöÄ #collaboration #growth\n"
          ]
        }
      ],
      "source": [
        "text=\"Awesome brainstorming session with @AlexM and @SamanthaT! Excited for what‚Äôs next. Appreciate the valuable input @DevMaster üí°üöÄ #collaboration #growth\"\n",
        "#CODE HERE\n",
        "import re\n",
        "def filter(msg):\n",
        "    processed_msg = re.sub(r'@\\w+', '', msg).strip()\n",
        "    return processed_msg\n",
        "tweet_txt = \"Awesome brainstorming session with @AlexM and @SamanthaT! Excited for what‚Äôs next. Appreciate the valuable input @DevMaster üí°üöÄ #collaboration #growth\"\n",
        "filtered_txt = filter(tweet_txt)\n",
        "print(filtered_txt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pQgvZufJriy"
      },
      "source": [
        "###This script is using re.sub() to remove usernames' Twitter handles (@AlexM, @SamanthaT, @DevMaster) within a tweet. It is using @ followed by alphabets or numerals (@\\w+) to replace anything that is preceded by @, by nothing. The .strip() is ensuring that no spaces are added after that. The tweet after cleaning is printed, keeping everything else including hashtags and emojis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyRZ4X3Aucy1"
      },
      "source": [
        "**Question 4:**\n",
        "\n",
        "##In many NLP tasks, especially tasks involving large documents, it's important to not just tokenize words but also segment sentences. Write a Python script to segment a multi-paragraph text into individual sentences, considering the following cases:\n",
        "\n",
        "Sentences ending with abbreviations like \"Dr.\", \"Mr.\", etc.\n",
        "Sentences that contain dialogue within quotation marks.\n",
        "Use spaCy or NLTK for sentence segmentation and briefly explain how contextual awareness impacts this process."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"\"\"Dr. Smith walked into the room. \"Hello, Mr. Johnson,\" he said. \"How are you today?\"\n",
        "Mr. Johnson replied, \"I'm fine, Dr. Smith. Thank you for asking.\"\n",
        "He then continued, \"Let's discuss the test results.\" The doctor nodded.\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "# Print results\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUYGOHUhKBpZ",
        "outputId": "c4abac27-4d8e-4b50-e2ac-cde4053fc191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Dr. Smith walked into the room.\n",
            "2. \"Hello, Mr. Johnson,\" he said.\n",
            "3. \"How are you today?\" \n",
            "\n",
            "4. Mr. Johnson replied, \"I'm fine, Dr. Smith.\n",
            "5. Thank you for asking.\n",
            "6. \"\n",
            "He then continued, \"Let's discuss the test results.\"\n",
            "7. The doctor nodded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvGnXMtpx0mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac21b67a-1cf0-4312-8fc4-5aa179044d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv3DDOgFuhbe"
      },
      "outputs": [],
      "source": [
        "Input_example_text =\"Dr. Adams, a renowned scientist, stated, We will analyze the samples by 3 p.m. tomorrow.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J94AajdJriy"
      },
      "outputs": [],
      "source": [
        "pip install \"pydantic<2.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJ5E13hrJriy"
      },
      "outputs": [],
      "source": [
        "!python3 -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKWb3yFOJriy"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zME8pbf7KWZG"
      },
      "source": [
        "#**Task 2.  - Regular Expressions (30%)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTzJuleZJriz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGDnTc3IJriz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyUHXY1eJriz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRR6kMx1haKo"
      },
      "source": [
        "###**Regular Expressions**\n",
        "A regular expression, often abbreviated as regex, is a powerful and flexible tool for pattern matching and text manipulation. It consists of a sequence of characters that defines a search pattern, allowing you to perform various text-related tasks such as text validation, data extraction, text cleaning, and more. Regular expressions are used in programming languages and text editors and are constructed using a combination of regular characters, special characters, and metacharacters to specify search criteria. Learning to use regular expressions effectively can greatly enhance text processing tasks, making them a valuable skill in fields like natural language processing, data extraction, and data validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhp7XiXXh-CQ"
      },
      "source": [
        "Python includes a builtin module called `re` which provides regular expression matching operations (Click [here](https://docs.python.org/3/library/re.html) for the official module documentation). Once the module is imported into your code, you can use all of the available capabilities for performing pattern-based matching or searching using regular expressions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojyfJF7bymRl"
      },
      "source": [
        "\n",
        "\n",
        "##**Question - 1**\n",
        "\n",
        "##Write a Python script that uses regular expressions to extract all email addresses and URLs from a text. Ensure your script handles:\n",
        "\n",
        "##Multiple TLDs (e.g., .com, .org)\n",
        "##Edge cases like emails with numbers (e.g., john123@domain.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evyfHbBrveNw"
      },
      "outputs": [],
      "source": [
        "Input_text = \"For more details, contact us at support@myemail.com or visit https://example.org. Alternatively, you can email sales@company123.org.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlcFDE341Idg"
      },
      "outputs": [],
      "source": [
        "#CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17YpjyzUo8oq"
      },
      "source": [
        "\n",
        "##**Question- 2**\n",
        "\n",
        "##Implement a Python program to find URLs in the given string using Regular expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRsGKmmskWJT"
      },
      "outputs": [],
      "source": [
        "text= '<p>Contents :</p><a href=\"https://w3resource.com\">Python Examples</a><a href=\"http://github.com\">Even More Examples</a><a href=\"https://openai.com\">OpenAI Homepage</a><a href=\"https://docs.python.org\">Python Documentation</a>'\n",
        "##Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf1Pm9CHs1LM"
      },
      "source": [
        "## **Using regular expressions based pattern matching on real world text**\n",
        "\n",
        "For the purposes of demonstration, here's a dummy paragraph of text. A few observations here:\n",
        "* The text has multiple paragraphs with each paragraph having more than one sentence.\n",
        "* Some of the words are capitalized (first letter is in uppercase followed by lowercase letters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPLUtLxVtiTH"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Here is the First Paragraph and this is the First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the first paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
        "Now, it is the Second Paragraph and its First Sentence. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the second paragraph. this paragraph is ending now with a Fifth Sentence.\n",
        "Finally, this is the Third Paragraph and is the First Sentence of this paragraph. here is the Second Sentence. now is the Third Sentence. this is the Fourth Sentence of the third paragaraph. this paragraph is ending now with a Fifth Sentence.\n",
        "4th paragraph is not going to be detected by either of the regex patterns below.\n",
        "\"\"\"\n",
        "\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqmf8KSFtt2E"
      },
      "source": [
        "The following code block shows a regular expression that matches only those strings that:\n",
        "1. are at the start of a line and\n",
        "2. the string does not start with a number or a whitespace\n",
        "\n",
        "`re.findall()` finds all matches of the pattern in the text under consideration. The output is a list of strings that matched."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyYWQ5Tzttzt"
      },
      "source": [
        "Further, the regular expression defined below matches the words that are capitalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxA3QOHHtqOg"
      },
      "outputs": [],
      "source": [
        "##code block - 3\n",
        "re_pattern2 = r'[A-Z][a-z]+'\n",
        "print(re.findall(re_pattern2, text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0VUlW3RunCd"
      },
      "source": [
        "\n",
        "Following is a text excerpt on \"Inaugural Address\" taken from the website of the [Joint Congressional Committee on Inaugural Ceremonies](https://www.inaugural.senate.gov/inaugural-address/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRxR7M-NuFtR"
      },
      "outputs": [],
      "source": [
        "inau_text=\"\"\"The custom of delivering an address on Inauguration Day started with the very first Inauguration‚ÄîGeorge Washington‚Äôs‚Äîon April 30, 1789(04-30-1789). ex:-18.5. After taking his oath of office on the balcony of Federal Hall in New York City, Washington proceeded to the Senate chamber where he read a speech before members of Congress and other dignitaries. His second Inauguration took place in Philadelphia on March 4, 1793(03/04/1793), in the Senate chamber of Congress Hall. There, Washington gave the shortest Inaugural address on record‚Äîjust 135 words ‚Äîbefore repeating the oath of office.\n",
        "Every President since Washington has delivered an Inaugural address. While many of the early Presidents read their addresses before taking the oath, current custom dictates that the Chief Justice of the Supreme Court administer the oath first, followed by the President‚Äôs speech.\n",
        "William Henry Harrison delivered the longest Inaugural address, at 8,445 words, on March 4, 1841‚Äîa bitterly cold, wet day. He died one month later of pneumonia, believed to have been brought on by prolonged exposure to the elements on his Inauguration Day. John Adams‚Äô Inaugural address, which totaled 2,308 words, contained the longest sentence, at 737 words. After Washington‚Äôs second Inaugural address, the next shortest was Franklin D. Roosevelt‚Äôs fourth address on January 20, 1945(01-20-1945), at just 559.0 words. Roosevelt had chosen to have a simple Inauguration at the White House in light of the nation‚Äôs involvement in World War II.\n",
        "In 1921, Warren G. Harding became the first President to take his oath and deliver his Inaugural address through loud speakers. In 1925, Calvin Coolidge‚Äôs Inaugural address was the first to be broadcast nationally by radio. And in 1949, Harry S. Truman became the first President to deliver his Inaugural address over television airwaves.\n",
        "Most Presidents use their Inaugural address to present their vision of America and to set forth their goals for the nation. Some of the most eloquent and powerful speeches are still quoted today. In 1865, in the waning days of the Civil War, Abraham Lincoln stated, ‚ÄúWith malice toward none, with charity for all, with firmness in the right as God gives us to see the right, let us strive on to finish the work we are in, to bind up the nation‚Äôs wounds, to care for him who shall have borne the battle and for his widow and his orphan, to do all which may achieve and cherish a just and lasting peace among ourselves and with all nations.‚Äù In 1933, Franklin D. Roosevelt avowed, ‚Äúwe have nothing to fear but fear itself.‚Äù And in 1961, John F. Kennedy declared, ‚ÄúAnd so my fellow Americans: ask not what your country can do for you‚Äîask what you can do for your country.‚Äù\n",
        "Today, Presidents deliver their Inaugural address on the West Front of the Capitol, but this has not always been the case. Until Andrew Jackson‚Äôs first Inauguration in 1829, most Presidents spoke in either the House or Senate chambers. Jackson became the first President to take his oath of office and deliver his address on the East Front Portico of the U.S. Capitol in 1829. With few exceptions, the next 37.0 Inaugurations took place there, until 1981, when Ronald Reagan‚Äôs Swearing-In Ceremony and Inaugural address occurred on the West Front Terrace of the Capitol. The West Front has been used ever since. You should also need to extract the floating numbers such as -55.5, 20.8%, -3.0 using your regular expression\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qMOPAWe3wnj"
      },
      "source": [
        "Refer to  above code block -3 for the following questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Oap3GNEut1G"
      },
      "source": [
        "\n",
        "##**Questions-3.A**\n",
        "## Identify all the positive and neagtive numbers with type of both intergers,float  in the \"Inaugural Address\" excerpt and write a regular expression that finds all occurrences of such words in the text. Then, run the Python code snippet to automatically display the matched strings according to the pattern.*.\n",
        "\n",
        "NOTE: You can use the *re.findall()* method as demonstrated in the example before this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNqyUNbPurJm"
      },
      "outputs": [],
      "source": [
        "##Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWQdvlG0dPKy"
      },
      "source": [
        "##Your explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mkbLF4nu7la"
      },
      "source": [
        "##**Question-3.B**\n",
        "##*Identify all the dates of all forms - text form(April 20, 1945) and digit form(xx-xx-xxxx, xx/xx/xxxx) in the \"Inaugural Address\" excerpt and write a regular expression that finds all occurrences of the dates in the text. Then, run the Python code snippet to automatically display a list of all such dates identified.*\n",
        "\n",
        "NOTE: You can use the *re.findall()* method as demonstrated in the example before this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pxl72VhvEcG"
      },
      "outputs": [],
      "source": [
        "##Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdUT4vfidQ_L"
      },
      "source": [
        "##Your explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZSO1IrN06xI"
      },
      "source": [
        "#**Task 3: Lemmatization/Stemming(25%)**\n",
        "\n",
        "\n",
        "#**Question -1:**\n",
        "##How does the morphology of a language (e.g., agglutinative vs. fusional) impact the suitability of stemming vs. lemmatization?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ycUYp41Iy_"
      },
      "source": [
        "##Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMMf51DdsaK5"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        " ## **Question - 2 :**\n",
        "\n",
        "\n",
        "## Create a Python function that takes a sentence as input, performs lemmatization using **Stanza**, and removes stopwords from the lemmatized sentence.Return the cleaned and lemmatized sentence.\n",
        "\n",
        "Reference: https://github.com/stanfordnlp/stanza\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udt8qI_viOXf"
      },
      "outputs": [],
      "source": [
        "#CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri0OIiWfBN8R"
      },
      "source": [
        "##Your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7o1E0wTupIA"
      },
      "source": [
        "##**Question - 3**\n",
        "\n",
        "(Refer to the byte pair encoding concept which is explained in activity-3 at the Tutorial of Subword Tokenization using HuggingFace after the Question-6 in activity-3)\n",
        "\n",
        "\n",
        "Consider the following two sentences:\n",
        "\n",
        "**S1:** \"The artist is creating a better version of his masterpiece by refining the details.\"\n",
        "\n",
        "**S2:**\"She is bettering her artistic skills by creating new versions of her paintings.\"\n",
        "\n",
        "**Create a Python function that encodes two sentences using the custom BPE tokenizer and identifies common subword tokens (tokens that appear in both encodings). Return a list of these common subword tokens. Is/Are there any interesting observations when you compare the tokens between the two encodings? What do you think is causing what you observe as part of your comparison?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RY65-RPvURG"
      },
      "outputs": [],
      "source": [
        "#code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEKqJAYbvUG3"
      },
      "source": [
        "# **Task - 4 : Minimum Edit distance (25%)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##Minimum edit Distance\n",
        "Minimum Edit Distance (also known as Levenshtein Distance) is a measure of similarity between two strings by calculating the minimum number of single-character edits (insertions, deletions, substitutions) required to transform one string into the other. It has applications in various fields, including natural language processing, spell checking, DNA sequence alignment, and more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNZzSXu2xoAI"
      },
      "source": [
        "# Character Based Text Similarity\n",
        "\"As an example, this technology is used by information retrieval systems, search engines, automatic indexing systems, text summarizers, categorization systems, plagiarism checkers, speech recognition, rating systems, DNA analysis, and profiling algorithms (IR/AI programs to automatically link data between people and what they do).\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHA_ccV_xmCB"
      },
      "outputs": [],
      "source": [
        "##code block -4\n",
        "# A Naive recursive Python program to find minimum number\n",
        "# operations to convert str1 to str2\n",
        "\n",
        "\n",
        "def editDistance(str1, str2, m, n):\n",
        "\n",
        "    if m == 0:\n",
        "        return n\n",
        "\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if str1[m-1] == str2[n-1]:\n",
        "        return editDistance(str1, str2, m-1, n-1)\n",
        "\n",
        "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert\n",
        "                   editDistance(str1, str2, m-1, n),    # Remove\n",
        "                   editDistance(str1, str2, m-1, n-1)    # Replace\n",
        "                   )\n",
        "\n",
        "str1 = \"sunday\"\n",
        "str2 = \"saturday\"\n",
        "print (editDistance(str1, str2, len(str1), len(str2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSfpOff4Ngbx"
      },
      "source": [
        "Refer above code block -4  for the following question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUJ1n35-dgON"
      },
      "source": [
        "##Question-1\n",
        "##Assuming case sensitivity where changing a letter's case has a cost of 1, calculate the minimum cost to transform \"Educate\" into \"Education\" with the following operation costs: insertions = 3, deletions = 1, substitutions = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bt4ZjDL2c3LB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10ca234-fa33-4bab-c44a-7a35b98ecf15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted Minimum Edit Distance: 8\n"
          ]
        }
      ],
      "source": [
        "##your code here\n",
        "import numpy as np\n",
        "\n",
        "def weighted_edit_distance(str1, str2, ins_cost=3, del_cost=1, sub_cost=2, case_change_cost=1):\n",
        "    len1, len2 = len(str1), len(str2)\n",
        "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
        "\n",
        "    for i in range(len1 + 1):\n",
        "        dp[i][0] = i * del_cost\n",
        "    for j in range(len2 + 1):\n",
        "        dp[0][j] = j * ins_cost\n",
        "\n",
        "    for i in range(1, len1 + 1):\n",
        "        for j in range(1, len2 + 1):\n",
        "            if str1[i - 1] == str2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            elif str1[i - 1].lower() == str2[j - 1].lower():\n",
        "                dp[i][j] = dp[i - 1][j - 1] + case_change_cost\n",
        "            else:\n",
        "                dp[i][j] = min(\n",
        "                    dp[i - 1][j] + del_cost,  # Deletion cost\n",
        "                    dp[i][j - 1] + ins_cost,  # Insertion cost\n",
        "                    dp[i - 1][j - 1] + sub_cost  # Substitution cost\n",
        "                )\n",
        "\n",
        "    return dp[len1][len2]\n",
        "\n",
        "str1 = \"Educate\"\n",
        "str2 = \"Education\"\n",
        "distance = weighted_edit_distance(str1, str2, ins_cost=3, del_cost=1, sub_cost=2, case_change_cost=1)\n",
        "print(\"Weighted Minimum Edit Distance:\", distance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fdCkNgZdL7_"
      },
      "source": [
        "##Your explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFiLMn7rIHUd"
      },
      "source": [
        "##Tutorial -2\n",
        "# Levenshtein Distance for Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FypLc-dkO96G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8eab92-8754-4fea-d077-6bface0988ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Levenshtein distance between 'This is a cat' and 'That is a dog' with substitution cost 2 is 10\n"
          ]
        }
      ],
      "source": [
        "#code block - 5\n",
        "def levenshtein_distance(str1, str2):\n",
        "    m, n = len(str1), len(str2)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if str1[i - 1] == str2[j - 1] else 2  # Substitution cost is 2\n",
        "            dp[i][j] = min(\n",
        "                dp[i - 1][j] + 1,  # Deletion\n",
        "                dp[i][j - 1] + 1,  # Insertion\n",
        "                dp[i - 1][j - 1] + cost,  # Substitution\n",
        "            )\n",
        "\n",
        "    return dp[m][n]\n",
        "\n",
        "# Example usage\n",
        "str1 = \"This is a cat\"\n",
        "str2 = \"That is a dog\"\n",
        "distance = levenshtein_distance(str1, str2)\n",
        "print(f\"The Levenshtein distance between '{str1}' and '{str2}' with substitution cost 2 is {distance}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-xrzKVC5iSe"
      },
      "source": [
        "Refer to above code block -5 from tutorial - 2 for the following question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EapfLAq5iCLo"
      },
      "source": [
        "##Question:2\n",
        "##Assign different costs to insertions, deletions, and substitutions to reflect varying penalties for different types of edits. Calculate the Levenshtein distance with these weighted costs.\n",
        "\n",
        "String1 = (\"Natural language processing\")\n",
        "\n",
        "String2 = (\"Computer science department\")\n",
        "\n",
        "Provide your explanation in the tex block below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcDvlYTD4YvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5291d4f-f6fc-46af-e525-3d23cb2f1216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted Levenshtein Distance: 34\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def weighted_levenshtein(str1, str2, ins_cost=1, del_cost=1, sub_cost=2):\n",
        "    len1, len2 = len(str1), len(str2)\n",
        "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
        "\n",
        "    for i in range(len1 + 1):\n",
        "        dp[i][0] = i * del_cost\n",
        "    for j in range(len2 + 1):\n",
        "        dp[0][j] = j * ins_cost\n",
        "\n",
        "    for i in range(1, len1 + 1):\n",
        "        for j in range(1, len2 + 1):\n",
        "            if str1[i - 1] == str2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = min(\n",
        "                    dp[i - 1][j] + del_cost,  # Deletion\n",
        "                    dp[i][j - 1] + ins_cost,  # Insertion\n",
        "                    dp[i - 1][j - 1] + sub_cost  # Substitution\n",
        "                )\n",
        "\n",
        "    return dp[len1][len2]\n",
        "\n",
        "# Example usage\n",
        "str1 = \"Natural language processing\"\n",
        "str2 = \"Computer science department\"\n",
        "\n",
        "distance = weighted_levenshtein(str1, str2, ins_cost=1, del_cost=1, sub_cost=2)\n",
        "print(\"Weighted Levenshtein Distance:\", distance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "texg77AFdG6A"
      },
      "source": [
        "##Your explanation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg7XS-bAztur"
      },
      "source": [
        "# Question -3\n",
        "\n",
        "##Modify the minimum edit distance algorithm to account for different penalties:\n",
        "\n",
        "Insertion = 1\n",
        "\n",
        "Deletion = 2\n",
        "\n",
        "Substitution = 3\n",
        "\n",
        "Apply it to the following sentences:\n",
        "\n",
        "**Senetence 1 = \"Data Science is evolving fast.**\"\n",
        "\n",
        "**Senetence 2 = \"Artificial Intelligence is transforming industries.**\"\n",
        "\n",
        "\n",
        "Explain how different weights reflect real-world penalties in tasks like document comparison or DNA sequence matching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjRSiKhj0avx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6271eb6-7036-4db9-a791-5298e0047c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted Minimum Edit Distance: 51\n"
          ]
        }
      ],
      "source": [
        "# code here\n",
        "import numpy as np\n",
        "\n",
        "def weighted_edit_distance(str1, str2, ins_cost=1, del_cost=2, sub_cost=3):\n",
        "    len1, len2 = len(str1), len(str2)\n",
        "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
        "\n",
        "    for i in range(len1 + 1):\n",
        "        dp[i][0] = i * del_cost\n",
        "    for j in range(len2 + 1):\n",
        "        dp[0][j] = j * ins_cost\n",
        "\n",
        "    for i in range(1, len1 + 1):\n",
        "        for j in range(1, len2 + 1):\n",
        "            if str1[i - 1] == str2[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]\n",
        "            else:\n",
        "                dp[i][j] = min(\n",
        "                    dp[i - 1][j] + del_cost,  # Deletion cost\n",
        "                    dp[i][j - 1] + ins_cost,  # Insertion cost\n",
        "                    dp[i - 1][j - 1] + sub_cost  # Substitution cost\n",
        "                )\n",
        "\n",
        "    return dp[len1][len2]\n",
        "\n",
        "sentence1 = \"Data Science is evolving fast.\"\n",
        "sentence2 = \"Artificial Intelligence is transforming industries.\"\n",
        "\n",
        "distance = weighted_edit_distance(sentence1, sentence2, ins_cost=1, del_cost=2, sub_cost=3)\n",
        "print(\"Weighted Minimum Edit Distance:\", distance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zLdz7KC0YbC"
      },
      "source": [
        "##Question-4\n",
        "##Explain how Minimum Edit Distance (MED) is applied in NLP tasks like spell checking, speech recognition, text summarization, and machine translation. How does its effectiveness differ across these tasks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfyAF_SacJZd"
      },
      "source": [
        "##Your Explanation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}